import os
import json
import boto3
import pinecone
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage

# Load environment variables
PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
PINECONE_ENV = os.getenv('PINECONE_ENVIRONMENT')
SAGEMAKER_ENDPOINT_NAME = os.getenv('SAGEMAKER_ENDPOINT_NAME')

# Initialize Pinecone
pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)
index_name = 'your_pinecone_index_name'
index = pinecone.Index(index_name)

# SageMaker client for generating embeddings
sagemaker_runtime = boto3.client('runtime.sagemaker')

# Prompt template for LLM
PROMPT_TEMPLATE = """
Answer the question using only the following context:
{context}
-------------------------------------------------------------
Based on the above context, answer this question:
{question}
"""

def embed_query_with_sagemaker(query_text):
    """Generate embeddings for the query using the SageMaker endpoint."""
    payload = {'inputs': [query_text]}
    response = sagemaker_runtime.invoke_endpoint(
        EndpointName=SAGEMAKER_ENDPOINT_NAME,
        ContentType="application/json",
        Body=json.dumps(payload)
    )
    result = json.loads(response['Body'].read().decode())
    embeddings = result['embeddings'][0]
    return embeddings

def query_knowledge_base(query_text):
    """Query the Pinecone index using embeddings generated by SageMaker."""
    query_embedding = embed_query_with_sagemaker(query_text)
    
    # Query Pinecone to retrieve relevant documents
    response = index.query(queries=[query_embedding], top_k=5, include_metadata=True)

    return response['matches']

def format_source_path(full_path):
    return os.path.basename(full_path)

def lambda_handler_query(event, context):
    """Lambda handler for processing and reranking queries."""
    try:
        # Extract query from the Lambda event input
        query_text = event.get('query', '')

        if not query_text:
            return {
                'statusCode': 400,
                'body': json.dumps('No query provided.')
            }

        # Query the knowledge base
        results = query_knowledge_base(query_text)

        if not results:
            return {
                'statusCode': 404,
                'body': json.dumps(f"No matching results for query: '{query_text}'")
            }

        # Prepare context for LLM
        context = "\n\n---\n\n".join([match['metadata']['text'][:300] for match in results])
        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
        prompt = prompt_template.format(context=context, question=query_text)

        # Use LLM to generate the answer
        model = ChatOpenAI()
        response = model.invoke([HumanMessage(content=prompt)])
        response_text = response.content

        # Compile response with answer and sources
        response_body = {
            "query": query_text,
            "answer": response_text,
            "sources": []
        }

        unique_sources = set()
        for match in results:
            source = format_source_path(match['metadata'].get("source", "Unknown"))
            if source not in unique_sources:
                unique_sources.add(source)
                relevant_excerpt = match['metadata'].get("text", "")[:200]
                response_body["sources"].append({
                    "source": source,
                    "excerpt": relevant_excerpt
                })

        # Return the final response with sources
        return {
            'statusCode': 200,
            'body': json.dumps(response_body)
        }

    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps(f"An error occurred: {str(e)}")
        }
